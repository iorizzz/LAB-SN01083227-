{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6d3835d-16ac-49a7-a968-b283e4c8a31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emier\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\emier\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\emier\\AppData\\Local\\Temp\\ipykernel_13356\\1509295096.py:26: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  df[\"html_removed\"] = df[\"urls_removed\"].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0  Im happy with uniten actually, even the people...   \n",
      "1  I’m having a pretty good time here, happy to m...   \n",
      "2        a very neutral place in terms of everything   \n",
      "3  I would say Uniten it's  a good university  bu...   \n",
      "4   UNITEN is well-regarded, particularly for its...   \n",
      "\n",
      "                                          lemmatized  \n",
      "0             im happy united actually even people w  \n",
      "1           i’m pretty good time happy meet w people  \n",
      "2                      neutral place term everything  \n",
      "3  would say united good university issue need im...  \n",
      "4  united wellregarded particularly strong engine...  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Step 1: Read the source data\n",
    "df = pd.read_csv(\"UNITENReview.csv\")\n",
    "\n",
    "# Step 2: Convert text to lowercase\n",
    "df[\"lowercased\"] = df[\"Review\"].apply(lambda x: x.lower())\n",
    "\n",
    "# Step 3: Remove URLs\n",
    "df[\"urls_removed\"] = df[\"lowercased\"].apply(lambda x: re.sub(r'http\\S+|www\\S+', '', x))\n",
    "\n",
    "# Step 4: Remove HTML tags\n",
    "df[\"html_removed\"] = df[\"urls_removed\"].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "\n",
    "# Step 5: Remove emojis\n",
    "df[\"emojis_removed\"] = df[\"html_removed\"].apply(lambda x: emoji.replace_emoji(x, replace=''))\n",
    "\n",
    "# Step 6: Replace internet slang/chat words\n",
    "slang_dict = {\n",
    "    \"tbh\": \"to be honest\", \"omg\": \"oh my god\", \"lol\": \"laugh out loud\", \"idk\": \"I don't know\",\n",
    "    \"brb\": \"be right back\", \"btw\": \"by the way\", \"imo\": \"in my opinion\", \"smh\": \"shaking my head\",\n",
    "    \"fy\": \"for your information\", \"np\": \"no problem\", \"ikr\": \"I know right\", \"asap\": \"as soon as possible\",\n",
    "    \"bff\": \"best friend forever\", \"gg\": \"good game\", \"hmu\": \"hit me up\", \"rofl\": \"rolling on the floor laughing\"\n",
    "}\n",
    "def replace_slang(text):\n",
    "    for slang, full_form in slang_dict.items():\n",
    "        text = re.sub(r'\\b' + re.escape(slang) + r'\\b', full_form, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "df[\"slangs_replaced\"] = df[\"emojis_removed\"].apply(replace_slang)\n",
    "\n",
    "# Step 7: Replace contractions\n",
    "contractions_dict = {\n",
    "    \"wasn't\": \"was not\", \"isn't\": \"is not\", \"aren't\": \"are not\", \"weren't\": \"were not\",\n",
    "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"didn't\": \"did not\", \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\", \"shouldn't\": \"should not\", \"wouldn't\": \"would not\",\n",
    "    \"won't\": \"will not\", \"haven't\": \"have not\", \"hasn't\": \"has not\", \"hadn't\": \"had not\",\n",
    "    \"i'm\": \"I am\", \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\", \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\", \"they're\": \"they are\", \"i've\": \"I have\", \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\", \"they've\": \"they have\", \"i'd\": \"I would\", \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\", \"she'd\": \"she would\", \"we'd\": \"we would\", \"they'd\": \"they would\",\n",
    "    \"i'll\": \"I will\", \"you'll\": \"you will\", \"he'll\": \"he will\", \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\", \"they'll\": \"they will\", \"let's\": \"let us\", \"that's\": \"that is\",\n",
    "    \"who's\": \"who is\", \"what's\": \"what is\", \"where's\": \"where is\", \"when's\": \"when is\",\n",
    "    \"why's\": \"why is\"\n",
    "}\n",
    "def replace_contractions(text):\n",
    "    for contraction, full_form in contractions_dict.items():\n",
    "        text = re.sub(r'\\b' + re.escape(contraction) + r'\\b', full_form, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "df[\"contractions_replaced\"] = df[\"slangs_replaced\"].apply(replace_contractions)\n",
    "\n",
    "# Step 8: Remove punctuation and special characters\n",
    "df[\"punctuations_removed\"] = df[\"contractions_replaced\"].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Step 9: Remove numbers\n",
    "df[\"numbers_removed\"] = df[\"punctuations_removed\"].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "# Step 10: Correct spelling mistakes\n",
    "spell = Speller(lang='en')\n",
    "df[\"spelling_corrected\"] = df[\"numbers_removed\"].apply(lambda x: spell(x))\n",
    "\n",
    "# Step 11: Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df[\"stopwords_removed\"] = df[\"spelling_corrected\"].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "\n",
    "# Step 12: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df[\"lemmatized\"] = df[\"stopwords_removed\"].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "# Save the result to a CSV file\n",
    "df.to_csv(\"Processed_UNITENReviews.csv\", index=False)\n",
    "\n",
    "# Display the final processed data\n",
    "print(df[[\"Review\", \"lemmatized\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e1b23-5357-445e-b3d1-ddf80b911342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
